{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like [Random decision forest models](https://en.wikipedia.org/wiki/Random_forest), which we covered in [another notebook](03-model-random-forest.ipynb), [gradient boosted trees](https://en.m.wikipedia.org/wiki/Gradient_boosting) work by training an *ensemble* of imprecise decision trees.  However, while individual trees in random decision forests [focus on different subsets of features](https://en.wikipedia.org/wiki/Bootstrap_aggregating) to reduce variance and avoid overfitting, gradient boosting trains new weak learners to focus on examples that were mispredicted in the existing ensemble.\n",
    "\n",
    "We will begin by loading in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"fraud-cleaned-sample.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our data set into two. One part will be used for training the model, and the other will be a testing set we can use to evaluate the model we train. We're dealing with time-series data, so we'll split the data set based on time.\n",
    "\n",
    "In order to save memory and time, we'll further downsample the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = df['timestamp'].min()\n",
    "last = df['timestamp'].max()\n",
    "cutoff = first + ((last - first) * 0.7)\n",
    "\n",
    "train = df[df['timestamp'] <= cutoff].sample(frac=0.35, random_state=404).copy()\n",
    "test = df[df['timestamp'] > cutoff].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also load in the feature engineering pipeline stage which we developed in [notebook 2](02-feature-engineering.ipynb). The model takes the feature vectors as input, rather than the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle as cp\n",
    "feature_pipeline = cp.load(open('feature_pipeline.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with Imbalanced Classes\n",
    "\n",
    "When the training data set contains unequal representation from each of your classes we say we are dealing with 'imbalanced classes'. In our data set fewer than 2% of the samples are fraudulent, and the remaining 98% are legitimate. Thus we have imbalanced classes. \n",
    "\n",
    "This causes problems for a few reasons:\n",
    "\n",
    "1. A model which classifies all transactions as 'legitimate' would be correct 98% of the time. This high accuracy can trick you into thinking that your model is working well, despite it just returning 'legitimate' for every sample it sees. \n",
    "2. Even if your model tries to learn patterns in the data, it may struggle to learn from the fraudulent transactions since there simply aren't enough of them.\n",
    "\n",
    "XGBoost will address this problem by weighting mispredicted classes more heavily automatically, and this is the approach we'll take.  We can also give XGBoost a hint to explicitly weight classes in training before it automatically identifies imbalance; we'll get counts of classes in the training data now so that we can weight them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_count = train[train[\"label\"] == \"fraud\"][\"label\"].count()\n",
    "legit_count = train[train[\"label\"] == \"legitimate\"][\"label\"].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to train our Random Forest model. The model is trained on the feature vectors (generated using our `feature_pipeline` from the previous notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svecs = feature_pipeline.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = fraud_count/legit_count\n",
    "pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "\n",
    "# set this to:\n",
    "#  - 'exact' for slow but precise, \n",
    "#  - 'hist' for faster and less precise, or\n",
    "#  - 'gpu_hist' for a GPU-optimized implementation of 'hist'\n",
    "\n",
    "XGB_TREE_METHOD='hist'\n",
    "\n",
    "xgb = XGBClassifier(tree_method=XGB_TREE_METHOD, \n",
    "                    # num_parallel_tree=16, \n",
    "                    n_estimators=1024, \n",
    "                    max_depth=3, \n",
    "                    colsample_bynode=0.3, \n",
    "                    colsample_bytree=0.3, \n",
    "                    subsample=0.3)\n",
    "\n",
    "xgb.fit(svecs, train[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation \n",
    "\n",
    "We need to validate our model to check how well it performs on data it wasn't trained on. We use the model we just trained to make predictions for the data in our test set, and compare those predictions to the truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = xgb.predict(feature_pipeline.fit_transform(test))\n",
    "print(classification_report(test.label.values, predictions))\n",
    "class_report = classification_report(test.label.values, predictions, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report shows that the model is performing well and that it is slightly better at identifying legitimate transactions than fraudulent ones. \n",
    "\n",
    "We can visualise the classification accuracy in a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import plot\n",
    "df, chart = plot.binary_confusion_matrix(test[\"label\"], predictions)\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the raw counts, as well as the proportions of correctly and incorrectly classified items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting aspect of random decision forests is that they provide a metric for how important each feature was to the ultimate conclusion. This is a useful property both for having explainable models (i.e., so you can explain to a human why the model made a particular prediction) and for guiding further experiments (i.e., so you can learn more about the real world based on what the model has identified as likely to be correlated with what you're trying to predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(enumerate(xgb.feature_importances_))\n",
    "l.sort(key=lambda x: -x[1])\n",
    "l[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the [feature engineering notebook](02-feature-engineering.ipynb) to see specifically that these features are, in order of importance:\n",
    "- 0: interarrival time since the previous transaction\n",
    "- 5: a hashed merchant id\n",
    "- 6: a hashed merchant id\n",
    "- 2: a hashed merchant id\n",
    "- 3: a hashed merchant id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to save the model so that we can use it outside of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlworkflows import util\n",
    "xgb_dict = {'model': xgb, 'class_report': pd.DataFrame(class_report).transpose()}\n",
    "util.serialize_to(xgb_dict, \"xgb.sav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
